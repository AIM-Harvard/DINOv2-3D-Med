# DINOtxt (Image-Text Alignment) Training Configuration

vars:
    run_name: "DINOtxt_pretrain"
    img_size: [160, 160, 160]
    hidden_size: 864
    patch_size: [8, 8, 8]

trainer:
    _target_: pytorch_lightning.Trainer
    benchmark: true
    max_epochs: 1000
    accelerator: gpu
    strategy: ddp_find_unused_parameters_true
    enable_model_summary: false
    log_every_n_steps: 10
    limit_train_batches: 250
    fast_dev_run: false
    num_sanity_val_steps: 0
    precision: 16-mixed
    devices: 3
    detect_anomaly: false
    sync_batchnorm: true
    logger:
        _target_: pytorch_lightning.loggers.WandbLogger
        project: "dinov2"
        name: "%vars::run_name"
        save_dir: "$'/mnt/data1/suraj/dinov2_experiments/dinov2_pretrain/' + '%vars::run_name'"
    callbacks:
        - _target_: pytorch_lightning.callbacks.ModelCheckpoint
          dirpath: "$'/mnt/data1/suraj/dinov2_experiments/dinov2_pretrain/' + '%vars::run_name'"
          filename: "dinov2_3d_fixed_{epoch:03d}"
          save_last: true
          every_n_epochs: 1
          save_on_train_epoch_end: true

model:
    _target_: project.training.DINOtxt_LightningModule
    batch_size_per_device: 2
    hidden_size: "%vars::hidden_size"
    ibot_separate_head: true
    base_lr: 0.0001
    layer_decay: 0.9
    gradient_clip_val: 3.0
    teacher_temp_warmup_epochs: 30
    teacher_temp_min: 0.005
    teacher_temp_max: 0.01
    freeze_last_layer_epochs: 1
    projection_dim: 16384
    weight_decay: 0.04
    vision_encoder:
        _target_: project.models.backbones.vision_enc_wrapper.VisionEncoder_w_Blocks
        backbone:
            _target_: project.models.backbones.Primus
            embed_dim: "%vars::hidden_size"
            eva_depth: 16
            eva_numheads: 12
            input_channels: 1
            num_classes: 1
            input_shape: "%vars::img_size"
            patch_embed_size: "%vars::patch_size"
            patch_drop_rate: 0.0
            classification: true
    text_encoder:
        _target_: project.models.backbones.text_encoder.TextEncoder
        model_name: "openai/clip-vit-base-patch32"
        output_dim: "%vars::hidden_size"
        freeze_encoder: true
        use_projection: true
        pooling_strategy: "cls"
        max_length: 77
    backbone:

data:
    _target_: project.training.DataModule
    num_workers: 12
    batch_size: 2
    pin_memory: true
    drop_last: true
    train_dataset:
        _target_: project.utils.safe_dataset.SafeDataset
        dataset:
            _target_: monai.data.Dataset
            data:
            transform:
                _target_: torchvision.transforms.Compose
                transforms:
                    - _target_: monai.transforms.LoadImaged
                      keys: ["image"]
                      image_only: true
                    - _target_: monai.transforms.EnsureChannelFirstd
                      keys: ["image"]
                    - _target_: monai.transforms.Orientationd
                      keys: ["image"]
                      axcodes: SPL
                    - _target_: monai.transforms.Spacingd
                      keys: ["image"]
                      pixdim: [1.0, 1.0, 1.0]
                      mode: bilinear
                    - _target_: monai.transforms.CropForegroundd
                      keys: ["image"]
                      source_key: "image"
                    - _target_: monai.transforms.SpatialPadd
                      keys: ["image"]
                      spatial_size: "%vars::img_size"
                      value: -1024
                    - _target_: monai.transforms.ScaleIntensityRanged
                      keys: ["image"]
                      a_min: -1024
                      a_max: 2048
                      b_min: 0
                      b_max: 1
                      clip: true
                    - _target_: monai.transforms.RandSpatialCropd
                      keys: ["image"]
                      roi_size: "%vars::img_size"
                    - _target_: torchvision.transforms.Lambda
                      lambd: "$lambda x: x['image'].as_tensor()"
                    - _target_: project.transforms.dinov2_aug.DINOv2Augmentation3D
                      global_view_scale: [0.3, 1.0]
                      global_view_size: "$@vars::img_size[0]"
                      local_view_scale: [0.3, 1.0]
                      local_view_size: "$@vars::img_size[0]//2"
                      num_local_views: 0
                    - _target_: torchvision.transforms.Lambda
                      lambd: "$lambda x: (x, False)"
